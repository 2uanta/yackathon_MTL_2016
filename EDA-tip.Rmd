---
title: "EDA-tip"
author: "Quan Nguyen"
date: "March 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```
30,404,866 yelp_academic_dataset_business.csv
13,232,249 yelp_academic_dataset_checkin.csv
 1,643,584,468 yelp_academic_dataset_review.csv
74,489,432 yelp_academic_dataset_tip.csv
135,860,821 yelp_academic_dataset_user.csv
```

# Business

``` {r}
business = read.csv("yelp_academic_dataset_business.csv", header=T, strip.white = T, stringsAsFactors = F)
b = subset(business, select=c("business_id", "name", "state", "city", "categories"))
```

## Tip

```{r}
tip = read.csv("yelp_academic_dataset_tip.csv", header=T)
names(tip)
# [1] "user_id"     "text"        "business_id" "likes"       "date"       
# [6] "type" 
dim(tip)
# 591864      6
```

## Join

```{r}

tip_with_business = merge(tip, b, by="business_id")
QC = subset(tip_with_business, state=="QC")
QC = droplevels(QC)
dim(QC)
```


``` {r}
# Unique users in tip
dim(table(tip$user_id))
# [1] 117133
dim(table(QC$user_id))

# average tips per user
round(nrow(tip)/dim(table(tip$user_id)),0)
# [1] 5
head(sort(table(tip$user_id), decreasing=T),10)
round(head(sort(table(tip$user_id)/nrow(tip)*100, decreasing=T),10),2)
tail(sort(table(tip$user_id)/nrow(tip)*100, decreasing=T),10)
h = hist(log(table(tip$user_id)))
plot(h, xaxt = "n")
axis(1, at = pretty(h$breaks), labels = 10^pretty(h$breaks))

# average tips per user in QC
round(nrow(QC)/dim(table(QC$user_id)),0)
# [1] 3
head(sort(table(QC$user_id), decreasing=T),10)
round(head(sort(table(QC$user_id)/nrow(QC)*100, decreasing=T),10),2)
tail(sort(table(QC$user_id)/nrow(QC)*100, decreasing=T),10)
h = hist(log(table(tip$user_id)))
plot(h, xaxt = "n")
axis(1, at = pretty(h$breaks), labels = 10^pretty(h$breaks))


range(as.Date(tip$date))
# [1] "2009-04-15" "2016-01-07"

tip$year = cut(as.Date(tip$date), breaks='year')
table(tip$year)

QC$year = cut(as.Date(QC$date), breaks='year')
table(QC$year)

QC$day = cut(as.Date(QC$date), breaks='day')
table(QC$day)
plot(table(QC$day))

# 2015
tip_2015 = subset(tip_with_business, cut(as.Date(tip_with_business$date), breaks='year') == "2015-01-01")
dim(tip_2015)
tip_2015 = droplevels(tip_2015)

QC_2015 = subset(tip_2015, state == "QC")
QC_2015 = droplevels(QC_2015)

QC_2015$day = cut(as.Date(QC_2015$date), breaks='day')
table(QC_2015$day)
plot(table(QC_2015$day))
tips_2015_QC = table(QC_2015$day)
write.csv(tips_2015_QC, file="tips_2015_QC.csv")

# Unique users in tip
dim(table(tip_2015$user_id))
# average comments per user
round(nrow(tip_2015)/dim(table(tip_2015$user_id)),2)

table(tip$like)
table(tip_2015$like)
```

``` {r}
library(tm)
tip_corpus = Corpus(VectorSource(tip_2015$text))
tip_corpus = tm_map(tip_corpus, stripWhitespace)
tip_corpus = tm_map(tip_corpus, removeNumbers)
tip_corpus = tm_map(tip_corpus, tolower)
tip_corpus = tm_map(tip_corpus, removePunctuation)
# will take 5 minutes
tip_corpus = tm_map(tip_corpus, removeWords, stopwords("english"))

# Problem: cannot mix French and English stopwords
# tip_corpus <- tm_map(tip_corpus, removeWords, stopwords("french"))
tip_corpus <- tm_map(tip_corpus, stemDocument)
tip_corpus <- tm_map(tip_corpus, PlainTextDocument)

library(wordcloud)
# png("wordcloud.png", width=1280,height=800)
wordcloud(
  tip_corpus, 
  scale=c(6,.5), # difference between the largest and smallest font
  max.words=100, 
  min.freq = 2, # words with frequency below min.freq will not be plotted
  random.order=FALSE, 
  rot.per=0.2, #  percentage of vertical text
  use.r.layout=FALSE, 
  colors=brewer.pal(8, "Dark2")
)
# dev.off()
```

For QC

``` {r}
library(tm)
tip_corpus = Corpus(VectorSource(QC_2015$text))
tip_corpus = tm_map(tip_corpus, stripWhitespace)
tip_corpus = tm_map(tip_corpus, removeNumbers)
tip_corpus = tm_map(tip_corpus, tolower)
tip_corpus = tm_map(tip_corpus, removePunctuation)
# will take 5 minutes
tip_corpus = tm_map(tip_corpus, removeWords, stopwords("english"))

# Problem: cannot mix French and English stopwords
# tip_corpus <- tm_map(tip_corpus, removeWords, stopwords("french"))
tip_corpus <- tm_map(tip_corpus, stemDocument)
tip_corpus <- tm_map(tip_corpus, PlainTextDocument)

library(wordcloud)
# png("wordcloud.png", width=1280,height=800)
wordcloud(
  tip_corpus, 
  scale=c(6,.5), # difference between the largest and smallest font
  max.words=100, 
  min.freq = 2, # words with frequency below min.freq will not be plotted
  random.order=FALSE, 
  rot.per=0.2, #  percentage of vertical text
  use.r.layout=FALSE, 
  colors=brewer.pal(8, "Dark2")
)
# dev.off(){r}

```



``` {r eval=F}
# DTM
tip_dtm = DocumentTermMatrix(tip_corpus)
tip_dtm
# view first 5 docs & first 20 terms 
inspect(tip_dtm[1:5, 1:20])
#
small_tip_dtm = removeSparseTerms(tip_dtm, sparse=0.995)
small_tip_dtm 
# Organize terms by their frequency:
freq <- colSums(as.matrix(small_tip_dtm))   
length(freq)  
# an alternate view of term frequency
findFreqTerms(tip_dtm, lowfreq=50)
sorted_dtm = sort(freq,decreasing = T)
# or
tip_wf <- data.frame(word=names(freq), freq=freq)   
head(tip_wf)
# Plot words that appear at least 2500 times.
library(ggplot2)   
p <- ggplot(subset(tip_wf, freq>2500), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p   
# m = as.matrix(small_tip_dtm)   
# dim(m)   
# write.csv(m, file="small_tip_dtm.csv")  
# Term Correlations
findAssocs(small_tip_dtm, c("staff" , "friendly"), corlimit=0.1) # specifying a correlation limit of 0.98   
findAssocs(small_tip_dtm, c("food"), corlimit=0.1) # specifying a correlation limit of 0.98 


# TDM
#
tip_tdm = TermDocumentMatrix(tip_corpus) 
tip_dtm
# view first 5 docs & first 20 terms 
inspect(tip_tdm[1:5, 1:20])

# Hierarchal Clustering
library(cluster)   
smaller_tip_dtm = removeSparseTerms(tip_dtm, sparse=0.97)
smaller_tip_dtm
freq <- colSums(as.matrix(smaller_tip_dtm))   
length(freq) 

d <- dist(t(smaller_tip_dtm), method="euclidian")   
fit <- hclust(d=d, method="ward")   
fit 
plot(fit, hang=-1)   
# 
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 5 clusters   
# k-means
library(fpc)   
d <- dist(t(smaller_tip_dtm), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)   
```

